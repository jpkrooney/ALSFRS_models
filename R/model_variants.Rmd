---
title: "Ordinal model variants"
output: html_document
---

```{r setup, message=FALSE, warning = FALSE, results = "hide"}
library(brms)
library(tidyverse)
source(here::here("R", "pp_checks.R"))
options(mc.cores = parallel::detectCores(), brms.backend = "cmdstanr")
cache_dir <- here::here("local_temp_data")
if(!dir.exists(cache_dir)) {
  dir.create(cache_dir)
}
```

Here I list some ideas how  to implement two proposed aspects of the model: multiple latent dimensions and having the ordinal thresholds vary per question. I do not make any conclusions yet, just exploring what is easily possible.

In all of the examples we'll work assuming we have just 3 questions for simplicity, we'll call them `q1, q2, q3`. 
Let's generate a very simple dataset for tests:

```{r}
rcumulative_logit <- function(N, mean, thresholds) {
  raw <- rlogis(N) + mean
  res <- rep(1, N)
  for(t in thresholds) {
    res <- res + (t < raw)
  }
  res
}

```


```{r}
set.seed(58224522)
N_patients <- 7
simple_test <- data.frame(patient = factor(1:N_patients)) %>%
  crossing(time_from_diagnosis = c(0, 3, 5)) %>%
  mutate(q1 = rcumulative_logit(n(), rnorm(N_patients)[as.integer(patient)] + time_from_diagnosis * 0.5, thresholds = c(-1,-0.3,0,1)),
         q2 = rcumulative_logit(n(), rnorm(N_patients, sd = 0.5)[as.integer(patient)] + time_from_diagnosis * 0.1, thresholds = c(-0.4,-0.1,0.5,1.5)),
         q3 = rcumulative_logit(n(), rnorm(N_patients, sd = 2)[as.integer(patient)] + time_from_diagnosis * 0.3, thresholds = c(-2,-0.8,-0.2,1.1)),
         obs_id = 1:n()
  )

simple_test
```

In all of the examples below we'll try to model the questions as dependent on `time_from_diagnosis`, and allowing between-patient variability.


For many of the models it will be useful to move to the long format, so here we go:

```{r}
simple_test_long <- simple_test %>% 
  pivot_longer(starts_with("q"), names_to = "question", values_to = "answer")
```


# Multiple latent dimensions

A simple baseline approach is to just model each question as a completely separate model or - _almost_ equivalently - to include full interactions between all effects and questions. Assuming only fixed effects, the math would be:

$$
\mu_{n,q} = \sum_{k = 1}^K { X_{n,k}\beta_{q,k} }  \\
Y_{n,q} \sim F(\mu_{n,q})
$$
$n$ iterates over subjects, $k$ over predictors and $q$ over questions. $F$ is the response distribution (cumulative logit in our case, but could be other). This is obviously very easy to implement in `brms`, but will likely waste some of the information in the data.

An approach that looks appealing, but is IMHO wrong is to model between-question correlations as purely part of the response distribution, (e.g. assuming multivariate normal response distribution - if our response was continuous). This would replace the response part as


$$
Y_{n} \sim F(\mu_{n}, \Sigma)
$$
where $\mu_n$ and $Y_n$ are the whole vectors over all questions and $\Sigma$ is some form of correlation structure.

I think we don't want this because we expect the repeated observations from the same patient over time to preserve the noise structure. E.g. assuming there is a negative correlation between bulbar and motor subscores in early disease, we would expect a patient that starts higher than average on bulbar subscore and also lower than average on the motor subscore to also be more likely in the same extremes when measured later (relative to average of patients at the later progression time). However, if there are other reasons to prefer this approach, I find it likely that it would be possible to implement it as a (hacky and weird) custom response distribution in `brms` by packing all the 12 answers into one integer, but I didn't try to actually do it.


I currently think the most reasonable way to model the dependencies is to make the $\beta$ (or some of the beta) correlated across questions:

$$
\beta_{q,k} = \eta_k + u_{q,k} \\
u_{,k} \sim MVN(0, \Sigma)
$$
Where $\eta_k$ is the "main" effect and $u_{q,k}$ are the "interaction" effects. 
It would also make sense to decompose $\Sigma$ into sds and  a correlation matrix and have the correlation matrix shared across all effects.

Unfortunately this form of correlation is not fully possible with `brms`. 
The closest possible thing is to have between-question correlations for a single grouping 
(e.g. as patient-specific varying intercepts):

```{r}
fit_dimensions1 <- brm(answer ~ question + (0 + question  | patient) + time_from_diagnosis, family = cumulative("logit"), data = simple_test_long, file = paste0(cache_dir, "/dimensions1.rds"), file_refit = "on_change")
fit_dimensions1
```

However this does not work for continuous predictors and it is not possible to have multiple coefficients share the correlation structure - i.e. if I added another `(0 + question | group)` term, it will have its own correlation matrix across questions. Still this could be a good start as it is easy to implement.

For a different project I actually already implemented and validated a linear model code in Stan that enforces one correlation matrix for all effects across questions about a year ago. So if we wanted to use a custom Stan code, I do have some building blocks ready.

# Making thresholds variable

## Modelling questions separately

A direct way to do this is by treating the questions as separate outcomes, either modelling each question separately or in a multivariate model, like this:

```{r}
fit_multivariate <- brm(mvbind(q1,q2,q3) ~ 1 + time_from_diagnosis + (1 | p | patient), data = simple_test, family = cumulative("logit"), file = paste0(cache_dir, "/multivariate.rds"), refresh = 0)
fit_multivariate
```

```{r}
pred_multivariate <- posterior_predict(fit_multivariate)
pp_check_cor_wide(pred_multivariate, simple_test, paste0("q", 1:3))
pp_check_cor_wide(pred_multivariate, simple_test, paste0("q", 1:3), group = rep(1:2, length.out = 21))
```


The obvious downside is that this becomes somewhat unwieldy for 24 questions (but this can be easily managed by generating formulas in code). The bigger downside is that now fixed effects (e.g. `time_from_diagnosis`) act separately on each question. Varying intercepts/effects can be made correlated across questions. We may even create a dummy variable with only one level to let us have all fixed effects as varying effects (and thus have correlation structure) - I remember someone being successful with that approach on the forums (https://discourse.mc-stan.org/t/model-both-response-and-between-group-correlations-multivariate-brms/19600), but it looks a bit fishy and produces unnecessary `sd_` parameters. 

## Using continuation ratio

We can move from a "cumulative" ordinal model (latent continuous dimension broken into segments by thresholds) to a "continuation ratio" ordinal model (we model a set of probabilities of the form "moving to level N, given that N - 1 was reached) - both models described in more detail in the [Ordinal Regression Models in Psychology: A Tutorial](https://psyarxiv.com/x8swp/) under "Sequential model". The "continuation ratio" (`cratio` in `brms`) seems theoretically somewhat less appealing, but not terrible - considering each question as a set of steps the patient takes one at a time is IMHO not completely unreasonable. The advantage of the `cratio` family is that it allows for "category-specific effects" (via a `cs()` term) that alter the category-specific intercepts. Those "category-specific effects" are not allowed for the `cumulative` family. As will become clearer later, they don't have trivial implementation for the `cumulative` family.

The model could then look like this:

```{r}
fit_cratio <- brm(answer ~ time_from_diagnosis + cs(question) + (1 + question | patient), family = cratio("logit"), data = simple_test_long, file = paste0(cache_dir, "/cratio.rds"), refresh = 0)
fit_cratio
```

```{r}
pred_cratio <- posterior_predict(fit_cratio)

pp_check_cor_long(pred_cratio, simple_test_long, "answer", "question", "obs_id")
pp_check_cor_long(pred_cratio, simple_test_long, "answer", "question", "obs_id", group = as.integer(simple_test_long$patient) < 4)

```


The biggest advantage definitely is that we don't need any tweaks to `brms`. 
This formulation also reframes the discussion about individual "dimensions" of progression - in the `cratio` model the coefficients act on the log-odds scale for progressing into the next stage (for the given question). There is no longer an underlying latent "progression", instead, there is "tendency to progress through a stage"... Not sure if this is actually a better way of thinking about the disease or a problematic assumption. Paul claims in his work that in most cases, you get the same inferences regardless of the choice of the ordinal model.

## Using `thres`

```{r}
fit_thres <- brm(answer | thres(gr = question) ~ time_from_diagnosis + (1 | patient), family = cumulative("logit"), data = simple_test_long, file = paste0(cache_dir, "/cumulative_thresh.rds"), refresh = 0)
fit_thres
```

```{r}
pred_thres <- posterior_predict(fit_thres)

pp_check_cor_long(pred_thres, simple_test_long, "answer", "question", "obs_id", actual_point_size = 10) 
pp_check_cor_long(pred_thres, simple_test_long, "answer", "question", "obs_id", group = as.integer(simple_test_long$patient) < 4)

```

## Using a custom family

Now why doesn't `brms` support the `cs` term for the `cumulative` family? Unlike `cratio` where the intercepts are independent and unconstrained, `cumulative` requires the intercepts to be ordered. It is not immediately clear how to make a predictor for an ordered vecxx tor and maintain the ordering constraint. What one may do is to look at the transformation Stan uses to represent ordered vectors (as described at https://mc-stan.org/docs/2_25/reference-manual/ordered-vector.html) and make predictors take effect on the _unconstrained_ variables. 

This however creates a somewhat weird situation, because the first intercept is untransformed while all the other intercepts are on the log scale, so the effects on the first intercept work fundamentally differently from the others. But as long as we stick to categorical predictors and wide priors, this is probably not a huge problem - categorical predictors mean there is a different value for each predictor level anyway, so the scale is less important.

With that we can build a custom family with 5 parameters - the main linear predictor, the first intercept (unconstrained threshold) and the 3 other intercepts defined as logarithm of the distance from the previous threshold. 

```{r}
cumulative_logit_cs5 <- custom_family(
  "cumulative_logit_cs5", dpars = c("mu", "threshA", "widthB", "widthC", "widthD"),
  links = c("identity", "identity", "log", "log", "log"), lb = c(NA, NA, 0, 0, 0),
  type = "int"
)

stan_funs <- "
  // The dummy parameter is required, probably due to a bug in brms (I don't need it with fewer than 4 distributional params...)
  real cumulative_logit_cs5_lpmf(int y, real mu, real thresh1, real width2, real width3, real width4, real dummy) {
    vector[4] params = to_vector({thresh1, width2, width3, width4});
    vector[4] thresholds = cumulative_sum(params);
    return ordered_logistic_lpmf(y | mu, thresholds);
  }
"
stanvars <- stanvar(scode = stan_funs, block = "functions")

# At least somewhat constraining priors are needed for the fit to work
prior_custom <-  c(set_prior("student_t(3, 0, 2.5)", class = "b", dpar = "threshA"),
          set_prior("normal(0.5, 2)", class = "b", dpar = "widthB"),
          set_prior("normal(0.5, 2)", class = "b", dpar = "widthC"),
          set_prior("normal(0.5, 2)", class = "b", dpar = "widthD")
          )


fit_custom <- brm(bf(answer ~ 0 + time_from_diagnosis + (1 + question | patient),
                     threshA ~ 0 + Intercept + question, # Avoiding centering of the intercept
                     widthB  ~ 0 + Intercept + question, 
                     widthC  ~ 0 + Intercept + question,
                     widthD  ~ 0 + Intercept + question),
                  family = cumulative_logit_cs5, data = simple_test_long, stanvars = stanvars, 
                  prior = prior_custom,
                  file = paste0(cache_dir, "/custom.rds"), 
                  silent = TRUE,
                  refresh = 0)


fit_custom
```

As implemented above, things like `loo` or `posterior_predict` won't work for the model, but this can be easily resolved with a bit more code.

This approach gives us the same flexibility as the `cratio` approach, but we can keep the "latent continuous progression" interpretation.


# Things left out

I didn't include how to use the "implied Dirichlet" prior for any of the models (as suggested in Mike's blog post), but I believe it should be possible if needed (especially to take care of questions where the full range of the answers is not present in the data)

